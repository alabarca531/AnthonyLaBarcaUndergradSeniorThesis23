@inproceedings{fukushima1979,
  abstract  = {In this paper, I propose a new algorithm for self-organizing a multilayered neural network which has an ability to recognize patterns based on the geometrical similarity of their shapes. This network, whose nickname is "neo-cognitron", has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of a photoreceptor layer followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells", which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The input synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We don't need any "teacher" during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer. The network has been simulated on a digital computer. After completion of self-organization, the stimulus patterns has become to elicit their own response from the last C-cell layer. That is, the response of the last C-cell layer changes without fail, if a stimulus patterns of a different category is presented to the input layer. The response of that layer, however, is not affected by the pattern's position at all. Neither is it affected by a certain amount of changes of the pattern's shape or size.},
  author    = {Fukushima, Kunihiko},
  location  = {Tokyo, Japan},
  publisher = {Morgan Kaufmann Publishers Inc.},
  booktitle = {Proceedings of the 6th International Joint Conference on Artificial Intelligence - Volume 1},
  date      = {1979},
  doi       = {10.5555/1624861.1624928},
  isbn      = {0934613478},
  pages     = {291--293},
  series    = {IJCAI'79},
  title     = {Self-Organization of a Neural Network Which Gives Position-Invariant Response},
}

@article{cybenko1989,
  abstract     = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  author       = {Cybenko, G.},
  url          = {https://doi.org/10.1007/BF02551274},
  date         = {1989-12-01},
  doi          = {10.1007/BF02551274},
  isbn         = {1435-568X},
  journaltitle = {Mathematics of Control, Signals and Systems},
  number       = {4},
  pages        = {303--314},
  title        = {Approximation by superpositions of a sigmoidal function},
  volume       = {2},
}

@inproceedings{krizhevsky2012,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor    = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  booktitle = {Advances in Neural Information Processing Systems},
  date      = {2012},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  volume    = {25},
}

@inproceedings{lecun2004,
  author    = {LeCun, Y. and Huang, Fu Jie and Bottou, L.},
  booktitle = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
  date      = {2004},
  doi       = {10.1109/CVPR.2004.1315150},
  pages     = {II--104 Vol.2},
  title     = {Learning methods for generic object recognition with invariance to pose and lighting},
  volume    = {2},
}

@article{parks2018,
  abstract     = {{We have designed, developed, and applied a convolutional neural network (CNN) architecture using multi-task learning to search for and characterize strong H i Ly$\alpha$ absorption in quasar spectra. Without any explicit modelling of the quasar continuum or application of the predicted line profile for Ly$\alpha$ from quantum mechanics, our algorithm predicts the presence of strong H i absorption and estimates the corresponding redshift zabs and H i column density \\$N\_\\{\\rm H\\,\\small \\{I\\}\\}\\$, with emphasis on damped Ly$\alpha$ systems (DLAs, absorbers with \\$N\_\\{\\rm H\\,\\small \\{I\\}\\}\\ge 2 \\times 10^\\{20\\} \\, \\{\\rm cm^\\{-2\\}\\}\\$). We tuned the CNN model using a custom training set of DLAs injected into DLA-free quasar spectra from the Sloan Digital Sky Survey (SDSS), data release 5 (DR5). Testing on a held-back validation set demonstrates a high incidence of DLAs recovered by the algorithm (97.4 per cent as DLAs and 99 per cent as an H i absorber with \\$N\_\\{\\rm H\\,\\small \\{I\\}\\}\\&gt; 10^\\{19.5\\} \\, \\{\\rm cm^\\{-2\\}\\}\\$) and excellent estimates for zabs and \\$N\_\\{\\rm H\\,\\small \\{I\\}\\}\\$. Similar results are obtained against a human-generated survey of the SDSS DR5 data set. The algorithm yields a low incidence of false positives and negatives but is challenged by overlapping DLAs and/or very high \\$N\_\\{\\rm H\\,\\small \\{I\\}\\}\\$ systems. We have applied this CNN model to the quasar spectra of SDSS DR7 and the Baryon Oscillation Spectroscopic Survey (data release 12) and provide catalogues of 4913 and 50 969 DLAs, respectively (including 1659 and 9230 high-confidence DLAs that were previously unpublished). This work validates the application of deep learning techniques to astronomical spectra for both classification and quantitative measurements.}},
  author       = {Parks, David and Prochaska, J Xavier and Dong, Shawfeng and Cai, Zheng},
  url          = {https://doi.org/10.1093/mnras/sty196},
  date         = {2018-01},
  doi          = {10.1093/mnras/sty196},
  eprint       = {https://academic.oup.com/mnras/article-pdf/476/1/1151/24261051/sty196.pdf},
  issn         = {0035-8711},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  number       = {1},
  pages        = {1151--1168},
  title        = {{Deep learning of quasar spectra to discover and characterize damped Ly$\alpha$ systems}},
  volume       = {476},
}

@inproceedings{vaswani2017,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  editor    = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  booktitle = {Advances in Neural Information Processing Systems},
  date      = {2017},
  title     = {Attention is All you Need},
  volume    = {30},
}

@article{dosovitskiy2020,
  author       = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  url          = {https://arxiv.org/abs/2010.11929},
  date         = {2020},
  eprint       = {2010.11929},
  eprinttype   = {arXiv},
  journaltitle = {CoRR},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  volume       = {abs/2010.11929},
}

@article{popescu2009,
  author       = {Popescu, Marius-Constantin and Balas, Valentina and Perescu-Popescu, Liliana and Mastorakis, Nikos},
  date         = {2009-07},
  journaltitle = {WSEAS Transactions on Circuits and Systems},
  title        = {Multilayer perceptron and neural networks},
  volume       = {8},
}

@misc{neutelings2021_nn,
  author       = {Neutelings, Izaak},
  publisher    = {TikZ.net},
  url          = {https://tikz.net/neural_networks/},
  date         = {2021-09-26},
  journaltitle = {TikZ.net},
  title        = {Neural networks},
}

@misc{neutelings2022_conv,
  author       = {Neutelings, Izaak},
  publisher    = {TikZ.net},
  url          = {https://tikz.net/conv2d/},
  date         = {2022-04-09},
  journaltitle = {TikZ.net},
  title        = {Neural networks},
}
   
@article{Naskath2022,
	abstract = {Deep learning is a wildly popular topic in machine learning and is structured as a series of nonlinear layers that learns various levels of data representations. Deep learning employs numerous layers to represent data abstractions to implement various computer models. Deep learning approaches like generative, discriminative models and model transfer have transformed information processing. This article proposes a comprehensive review of various deep learning algorithms Multi layer perception, Self-organizing map and deep belief networks algorithms. It first briefly introduces historical and recent state-of-the-art reviews with suitable architectures and implementation steps. Moreover, the various applications of those algorithms in various fields such as wireless networks, Adhoc networks, Mobile ad-hoc and vehicular ad-hoc networks, speech recognition engineering, medical applications, natural language processing, material science and remote sensing applications, etc. are classified.},
	author = {Naskath, J. and Sivakamasundari, G. and Begum, A. Alif Siddiqua},
	date = {2023/02/01},
	date-added = {2023-05-02 16:26:44 -0400},
	date-modified = {2023-05-02 16:27:15 -0400},
	doi = {10.1007/s11277-022-10079-4},
	id = {Naskath2023},
	isbn = {1572-834X},
	journal = {Wireless Personal Communications},
	month = {10},
	number = {4},
	pages = {2913--2936},
	title = {A Study on Different Deep Learning Algorithms Used in Deep Neural Nets: MLP SOM and DBN},
	url = {https://doi.org/10.1007/s11277-022-10079-4},
	volume = {128},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s11277-022-10079-4}}
 
 @misc{kumar2022_cnn, title={Different types of CNN Architectures explained: Examples}, url={https://vitalflux.com/different-types-of-cnn-architectures-explained-examples/}, journal={Data Analytics}, publisher={VitalFlux}, author={Kumar, Ajitesh}, year={2022}, month={04}} 

 @misc{he2016deep,
  doi = {10.48550/ARXIV.1512.03385},
  url = {https://arxiv.org/abs/1512.03385},
  author = {He,  Kaiming and Zhang,  Xiangyu and Ren,  Shaoqing and Sun,  Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Deep Residual Learning for Image Recognition},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@InProceedings{Simonyan15,
  author       = "Karen Simonyan and Andrew Zisserman",
  title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
  booktitle    = "International Conference on Learning Representations",
  year         = "2015",
}

@misc{Kim2014,
  doi = {10.48550/ARXIV.1408.5882},
  url = {https://arxiv.org/abs/1408.5882},
  author = {Kim,  Yoon},
  keywords = {Computation and Language (cs.CL),  Neural and Evolutionary Computing (cs.NE),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Convolutional Neural Networks for Sentence Classification},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}


@article{Kiranyaz2021,
	abstract = {During the last decade, Convolutional Neural Networks (CNNs) have become the de facto standard for various Computer Vision and Machine Learning operations. CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and millions of parameters have the ability to learn complex objects and patterns providing that they can be trained on a massive size visual database with ground-truth labels. With a proper training, this unique ability makes them the primary tool for various engineering applications for 2D signals such as images and video frames. Yet, this may not be a viable option in numerous applications over 1D signals especially when the training data is scarce or application specific. To address this issue, 1D CNNs have recently been proposed and immediately achieved the state-of-the-art performance levels in several applications such as personalized biomedical data classification and early diagnosis, structural health monitoring, anomaly detection and identification in power electronics and electrical motor fault detection. Another major advantage is that a real-time and low-cost hardware implementation is feasible due to the simple and compact configuration of 1D CNNs that perform only 1D convolutions (scalar multiplications and additions). This paper presents a comprehensive review of the general architecture and principals of 1D CNNs along with their major engineering applications, especially focused on the recent progress in this field. Their state-of-the-art performance is highlighted concluding with their unique properties. The benchmark datasets and the principal 1D CNN software used in those applications are also publicly shared in a dedicated website. While there has not been a paper on the review of 1D CNNs and its applications in the literature, this paper fulfills this gap.},
	author = {Serkan Kiranyaz and Onur Avci and Osama Abdeljaber and Turker Ince and Moncef Gabbouj and Daniel J. Inman},
	doi = {https://doi.org/10.1016/j.ymssp.2020.107398},
	issn = {0888-3270},
	journal = {Mechanical Systems and Signal Processing},
	keywords = {Artificial Neural Networks, Machine learning, Deep learning, Convolutional neural networks, Structural health monitoring, Condition monitoring, Arrhythmia detection and identification, Fault detection, Structural damage detection},
	pages = {107398},
	title = {1D convolutional neural networks and applications: A survey},
	url = {https://www.sciencedirect.com/science/article/pii/S0888327020307846},
	volume = {151},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0888327020307846},
	bdsk-url-2 = {https://doi.org/10.1016/j.ymssp.2020.107398}}

@misc{Huang2016,
  doi = {10.48550/ARXIV.1608.06993},
  url = {https://arxiv.org/abs/1608.06993},
  author = {Huang,  Gao and Liu,  Zhuang and van der Maaten,  Laurens and Weinberger,  Kilian Q.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Densely Connected Convolutional Networks},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{Shelhamer2016,
  doi = {10.48550/ARXIV.1605.06211},
  url = {https://arxiv.org/abs/1605.06211},
  author = {Shelhamer,  Evan and Long,  Jonathan and Darrell,  Trevor},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Fully Convolutional Networks for Semantic Segmentation},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}


@jurthesis{rubenzahl2018,
	address = {Rochester, NY},
	author = {Ryan Rubenzahl},
	date-added = {2023-05-02 20:19:47 -0400},
	date-modified = {2023-05-02 20:20:44 -0400},
	month = {May},
	school = {University of Rochester},
	title = {Identifying Type Ia Supernovae in Extragalactic Spectra},
	type = {Senior Thesis},
	year = {2018}}

@jurthesis{kitomi2019,
address = {Rochester, NY},
	author = {Ouail Kitomi},
	month = {May},
	school = {University of Rochester},
	title = {--},
	type = {Senior Thesis},
	year = {2019}}

 @jurthesis{Wasserman2020,
address = {Rochester, NY},
	author = {Amanda Wasserman},
	month = {May},
	school = {University of Rochester},
	title = {--},
	type = {Senior Thesis},
	year = {2020}}

  @jurthesis{Sepeku2022,
address = {Rochester, NY},
	author = {Edmund Sepeku},
	month = {May},
	school = {University of Rochester},
	title = {--},
	type = {Senior Thesis},
	year = {2022}}
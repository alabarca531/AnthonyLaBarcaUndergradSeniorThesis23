@inproceedings{fukushima1979,
  abstract  = {In this paper, I propose a new algorithm for self-organizing a multilayered neural network which has an ability to recognize patterns based on the geometrical similarity of their shapes. This network, whose nickname is "neo-cognitron", has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of a photoreceptor layer followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells", which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The input synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We don't need any "teacher" during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer. The network has been simulated on a digital computer. After completion of self-organization, the stimulus patterns has become to elicit their own response from the last C-cell layer. That is, the response of the last C-cell layer changes without fail, if a stimulus patterns of a different category is presented to the input layer. The response of that layer, however, is not affected by the pattern's position at all. Neither is it affected by a certain amount of changes of the pattern's shape or size.},
  author    = {Fukushima, Kunihiko},
  location  = {Tokyo, Japan},
  publisher = {Morgan Kaufmann Publishers Inc.},
  booktitle = {Proceedings of the 6th International Joint Conference on Artificial Intelligence - Volume 1},
  date      = {1979},
  doi       = {10.5555/1624861.1624928},
  isbn      = {0934613478},
  pages     = {291--293},
  series    = {IJCAI'79},
  title     = {Self-Organization of a Neural Network Which Gives Position-Invariant Response},
}

@article{cybenko1989,
  abstract     = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  author       = {Cybenko, G.},
  url          = {https://doi.org/10.1007/BF02551274},
  date         = {1989-12-01},
  doi          = {10.1007/BF02551274},
  isbn         = {1435-568X},
  journaltitle = {Mathematics of Control, Signals and Systems},
  number       = {4},
  pages        = {303--314},
  title        = {Approximation by superpositions of a sigmoidal function},
  volume       = {2},
}

@inproceedings{krizhevsky2012,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor    = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  booktitle = {Advances in Neural Information Processing Systems},
  date      = {2012},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  volume    = {25},
}

@inproceedings{lecun2004,
  author    = {LeCun, Y. and Huang, Fu Jie and Bottou, L.},
  booktitle = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
  date      = {2004},
  doi       = {10.1109/CVPR.2004.1315150},
  pages     = {II--104 Vol.2},
  title     = {Learning methods for generic object recognition with invariance to pose and lighting},
  volume    = {2},
}

@article{parks2018,
  abstract     = {{We have designed, developed, and applied a convolutional neural network (CNN) architecture using multi-task learning to search for and characterize strong H i Ly$\alpha$ absorption in quasar spectra. Without any explicit modelling of the quasar continuum or application of the predicted line profile for Ly$\alpha$ from quantum mechanics, our algorithm predicts the presence of strong H i absorption and estimates the corresponding redshift zabs and H i column density \\$N\_\\{\\rm H\\,\\small \\{I\\}\\}\\$, with emphasis on damped Ly$\alpha$ systems (DLAs, absorbers with \\$N\_\\{\\rm H\\,\\small \\{I\\}\\}\\ge 2 \\times 10^\\{20\\} \\, \\{\\rm cm^\\{-2\\}\\}\\$). We tuned the CNN model using a custom training set of DLAs injected into DLA-free quasar spectra from the Sloan Digital Sky Survey (SDSS), data release 5 (DR5). Testing on a held-back validation set demonstrates a high incidence of DLAs recovered by the algorithm (97.4 per cent as DLAs and 99 per cent as an H i absorber with \\$N\_\\{\\rm H\\,\\small \\{I\\}\\}\\&gt; 10^\\{19.5\\} \\, \\{\\rm cm^\\{-2\\}\\}\\$) and excellent estimates for zabs and \\$N\_\\{\\rm H\\,\\small \\{I\\}\\}\\$. Similar results are obtained against a human-generated survey of the SDSS DR5 data set. The algorithm yields a low incidence of false positives and negatives but is challenged by overlapping DLAs and/or very high \\$N\_\\{\\rm H\\,\\small \\{I\\}\\}\\$ systems. We have applied this CNN model to the quasar spectra of SDSS DR7 and the Baryon Oscillation Spectroscopic Survey (data release 12) and provide catalogues of 4913 and 50 969 DLAs, respectively (including 1659 and 9230 high-confidence DLAs that were previously unpublished). This work validates the application of deep learning techniques to astronomical spectra for both classification and quantitative measurements.}},
  author       = {Parks, David and Prochaska, J Xavier and Dong, Shawfeng and Cai, Zheng},
  url          = {https://doi.org/10.1093/mnras/sty196},
  date         = {2018-01},
  doi          = {10.1093/mnras/sty196},
  eprint       = {https://academic.oup.com/mnras/article-pdf/476/1/1151/24261051/sty196.pdf},
  issn         = {0035-8711},
  journaltitle = {Monthly Notices of the Royal Astronomical Society},
  number       = {1},
  pages        = {1151--1168},
  title        = {{Deep learning of quasar spectra to discover and characterize damped Ly$\alpha$ systems}},
  volume       = {476},
}

@inproceedings{vaswani2017,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  editor    = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  booktitle = {Advances in Neural Information Processing Systems},
  date      = {2017},
  title     = {Attention is All you Need},
  volume    = {30},
}

@article{dosovitskiy2020,
  author       = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  url          = {https://arxiv.org/abs/2010.11929},
  date         = {2020},
  eprint       = {2010.11929},
  eprinttype   = {arXiv},
  journaltitle = {CoRR},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  volume       = {abs/2010.11929},
}


\chapter{Deep Learning Techniques}
\label{chap:MLTechniques}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Outline 
% Introduce Machine Learning, and why traiditonal MLP is not good enough
% 1. Convolutional Neural Networks
% 1.1 Example architecture and sucess in vision tasks
% 1.2 1D CNN implemented on DESI spectra
% 1.3 2D CNN implemented on DESI spectra (Eddies Research)
% 2. Transformers 
% 2.1 Example architecture and sucess in NLP tasks + Vision Tasks --> Connection 
%      to DESI
% 2.2 Multi-Head Attention and Self-Attention for spectral classification
% 2.3 Possilbility of NOT having to k-correct for redshift because of this attention
% 2.4 Adjected ViT architecture for DESI

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multi-Layer Perceptrons}\label{sec:MLP}
To aid in the classification of supernovae, deep learning techniques can provide 
flexibility and scalability that have been previously unattainable by manual 
methods. The simplest and most widely used architecture in deep learning are the feed-forward 
fully connected neural networks, or multi-layer perceptrons (MLPs) \parencite{popescu2009}.
These networks are composed of series of ``layers'' of neurons, or nodes, that 
are ``fully connected'' to the previous layer (Figure \ref{fig:MLP}).
Given an input vector $\vec{x}^{(0)}$ with $n$ values, an MLP applies a series of weights to each 
element, resulting in a linear transformation from $\vec{x}^{(0)}$ to $\vec{x}^{(1)}$, where
$\vec{x}_{(1)}$ is a vector of length $m$. This transformation is then followed by a bias added 
to each element, and an activation function applied to each element.
The transition from layer $i$ to $i+1$ can be represented mathematically as 
\begin{equation}\label{eqn:MLP}
    \vec{x}^{(i+1)} = \sigma(\mathbf{W}\vec{x}^{(i)} + \vec{b}^{(i+1)}),
\end{equation}
where $\mathbf{W}$ is an $m \times n$ matrix of weights, $\vec{b}^{(i+1)}$ is a vector of length $m$
representing the bias, and $\sigma$ is the activation function. The activation function can 
be any function, but traditionally they are chosen to be continuous, non-linear, 
monotonically increasing, and differentiable. The most common activation functions 
are the sigmoid function, $\sigma(x_i) = \frac{1}{1 + e^{-x_i}}$, hyperbolic tangent, 
$\sigma(x_i) = \tanh(x_i)$, and the rectified linear unit, or ReLU,
$\sigma(x_i) = \max(0,x_i)$, all of which are applied element-wise.
\begin{figure}[t]
    \centering
    \input{tikz/MLP.tex}
    \caption{A simple MLP with an input dimension of 4 (green nodes), three hidden layers (blue nodes), and an output dimension of 3 (red nodes). Each node is connected to every 
    node in the previous and next layer. The lines represent the weights, and each 
    node has an associated bias. Figure adapted from \textcite{neutelings2021_nn}.}
    \label{fig:MLP}
\end{figure}
The output layer of the network is a vector tailored for the particular purpose
of the MLP. I.e. for classification, the output is a vector of normalized probabilities 
that the input data belongs to. 

The MLP architecture, while simplistic, is very powerful. \textcite{cybenko1989} showed that for any boolean 
function (i.e. 2-class classification), an MLP with a single hidden layer could 
approximate any function arbitrarily well; this is known as the Universal Approximation
Theorem. The theorem is the basis for the success of MLPs in
many fields, but it is also the reason that MLPs are not the best choice for all problems.
MLPs begin to show many complications during training large models, such as the 
exploding/vanishing gradient problem, a lack of invariance under translation, 
and extremely expensive backpropagation \parencite{Naskath2022}. These problems inhibit this architecture's 
abilities to scale to larger datasets and more complex problems, such as 
image recognition, time series analysis, and natural language processing. Since 
the mid-2000s, these problems have been addressed by the continued development of 
alternative architectures, such as convolutional neural networks (CNNs) and
transformers.

\section{Convolutional Neural Networks}\label{sec:CNN}
Convolutional neural networks (CNNs) were originally developed
to address the lack of invariance under translation in input data \parencite{fukushima1979}.
CNNs designed for classification are traditionally composed of a series of convolutional layers, 
usually followed by a pooling layer, and a fully connected layer. The convolutional 
layers are composed of a series of filters, which take a certain 
number of inputs from the previous layer and apply a kernel to them, resulting in 
a single output. These filters are across the entire input space, and are 
commonly referred to as feature maps. Inputs are traditionally padded to preserve 
the dimensionality of the original tensor size. Multiple filters are applied in 
each convolutional layer, resulting in a tensor of shape $N\times x$, where 
$x$ is the shape of the original input (which can be one or multi-dimensional). 
To reduce the dimensionality of the output, a pooling layer is applied.
The pooling layer takes a certain number of inputs from the previous layer, and 
reduces them to a single output, similar to the convolutional filters, but 
without the kernel application. The most common pooling layer is the max-pooling
layer, which takes the maximum value in each set as the reduced value. 
The process of passing an input through a convolutional filter, and then pooled is shown in Figure~\ref{fig:convolution}. 
These convolution and pooling layers are applied in series until the desired output dimensionality is reached.
\begin{figure}[t]
    \centering 
    \input{tikz/conn_unit}
    \caption{A convolutional filter and a $2\times2$ pooling applied to a 2D input. As the filter is passed 
        over the input space, the kernel is applied. The input is padded with zeros to preserve dimensionality, 
        which is then reduced by the max pooling. Figure adapted from \textcite{neutelings2022_conv}.}
    \label{fig:convolution}
\end{figure}

From here, depending on the task, the output of the pooling layer can be fed
into a fully connected layer, which is the same as the MLP architecture described
in Section~\ref{sec:MLP}, or it can be fed into another convolutional layer. An
MLP is usually applied for classification tasks, while another convolutional layer
is applied for segmentation tasks. An example of a CNN architecture used for 
classification is shown in Figure~\ref{fig:CNN}.

\begin{figure}[t]
    \centering
    \includegraphics[width=.8\linewidth]{figures/Typical-CNN-architecture.png}
    \caption{A convolutional network archeticture designed for classification (adapted from \cite{kumar2022_cnn}). Only
    one convolutional/pooling layer is shown, but there can be multiple depending on the application. }
    \label{fig:CNN}
\end{figure}


This architecture was shown to be effective in image recognition tasks by
\textcite{lecun2004} and then popularized after the success of AlexNet in the 
ImageNet challenge \textcite{krizhevsky2012}. AlexNet was the first of many CNN 
architectures popularized in the 2010s for vision classification, such as VGG \parencite{Simonyan15}, 
ResNet\parencite{he2016deep}, and DenseNet \parencite{Huang2016}. This also spurred a sudden increase in deep networks designed 
for segmentation tasks as well, such as the FCN architecture by \textcite{Shelhamer2016}. In addition to vision tasks, it also found 
great succes in applications such as natural language processing \parencite{Kim2014}, engineering, and 
medicine \parencite{Kiranyaz2021}. 

\subsection{CNNs on Spectroscopic Data in Astronomy}
\label{sec:CNNspectra}
CNNs excel at identifying patterns throughout their input space, stemming from their spacial invariance. 
Theoretically, this should translate to spectral classification quite well. In fact, CNNs have been applied 
to spectroscopic data in the past, including on the DESI dataset. 
\textcite{parks2018} used a CNN to detect strong emission lines in DESI spectra with 
great success. * Talk about 1D CNN currently running, and preprocessing used *

This work provides a baseline for the use of CNNs on supernovae classification, 
\textbf{but leaves more to be desired.} Therefore, an alternate architecture was proposed 
by * Cite Eddies work * which augments the preprocessing of the spectra with 
a conversion to a 2D image. This 2D image was then fed to a more traditional vision 
CNN architecture (Appendix~\ref{app:CNN}). 
% TODO: Make into Subcaptions
\begin{figure}[t]
    \centering
    \includegraphics[height=4.55cm]{figures/cnn/cnn_rocfull.png}
    \quad
    \includegraphics[height=4.55cm]{figures/cnn/cnn_cmfull.png}
    \caption{CNN Diagnostics: ROC Curve (left) and Confusion Matrix (right)\label{fig:cnn_qual}}
\end{figure}
This architecture was shown to train quickly (less than 1 hour on a single GPU), 
but it was not able to achieve astonishingly high accuracy. Figure~\ref{fig:cnn_qual}
shows the ROC curve and confusion matrix for the CNN trained on synthetic spectra. 
\begin{figure}[b]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/cnn/cnn_max_ypred.png}
    \caption{Max value of the output vector from the CNN.\label{fig:cnn_max}}
\end{figure}

The maximum value of the output vector was used to determine the predicted class,
\textbf{which may not be the best choice for this problem. why not?} Figure~\ref{fig:cnn_max} shows
the distribution of the maximum value found. As shown, there are approximately 
20 thousand spectra that are classified very confidently, but there are many
more that are not.
% TODO: Make into Subcaptions
\begin{figure}[t]
    \centering
    \includegraphics[height=4.55cm]{figures/cnn/cnn_roc99.png}
    \quad
    \includegraphics[height=4.55cm]{figures/cnn/cnn_cm99.png}
    \caption{CNN Diagnostics: ROC Curve (left) and Confusion Matrix (right) with a 99\% confidence cut\label{fig:cnn_qual2}}
\end{figure}

A \textbf{much better ROC curve and confusion matrix }are produced by 
evaluating the diagnostics for only highly confident classifications (Figure~\ref{fig:cnn_qual2}). 
Therefore, it is clear that the CNN, when confident in its classification, produces 
accurate results. This cut, however, is not ideal, removing *insert percent*\% of the 
data. 

The next step in training would be to either increase the confidence of the CNN 
or move to a different architecture, with the hopes of increasing not only 
the overall accuracy of the networks, but also the number of confident classifications.
% This begs the question, which is more important? The ability to classify a lot of 
% spectra with low confidence, or a few spectra with high confidence? Even if a 
% cut was made on the confidence of the classification, would the optimal solution
% yield better results under the same conditions? 

\section{Introduction of Transformers}\label{sec:transformers}
% Advantage of transformers in this field
Transformers are a relatively new architecture introduced in 2017 by \textcite{vaswani2017}
for natural language processing (NLP) tasks. The original architecture consists of 
an encoder-decoder system. The encoder accepts a series of tokens and produces a series of vectors
representing the input data via a series 
of self-attention layers and feed-forward layers. The decoder then takes the output of the encoder, and
produces a series of tokens, one for each input token. 

Once transformers were shown to have remarkable success in NLP tasks, they were 
quickly adapted to other fields, such as vision. \textcite{dosovitskiy2020} developed 
a vision transformer (ViT) architecture that differed from the original transformer 
encoder by replacing the tokenized input with a more involved preprocessing step. In short, 
the input image is broken into a series of patches, which are then flattened into
a vector. These vectors, along with positional encodings, are then fed into the
transformer architecture. For classification tasks, the first input token is 
replaced with a class token. After passing through the transformer, the class token 
is then run through a fully connected layer to produce the final probabilities. 

\subsection{ViT on Spectroscopic Data}\label{sec:ViT}
Previous implementations of transformers have been shown to have characteristics 
beneficial to the classification of spectral data. A transformer's 
ability to learn contextual information is essential in spectral classification. 
A broad absorption line, for example, may be indicative of a Type Ia supernova 
if in one part of the spectrum, but may be indicative of a Type II supernova if
in another part of the spectrum. This contextual information is not easily learned 
by a CNN, as the convolutional layers are not able to learn the importance of
certain parts of the input space. Attention can also play a role in identifying 
the purpose of features that are not in a standard location. For example, a 
\textbf{non-k corrected s}pectrum might have a continuum pattern at different locations 
in the spectrum, but the overall shape would be similar. This change in sizing 
would be difficult to learn with fixed filters in a CNN, but would be identified 
based on their positional importance by a transformer. In addition to this, ViTs 
have been shown to outperform CNNs on vision tasks, which shows they are capable 
of focusing on learned features. 


* Include caveat about the fact that ViTs take longer to train than CNNs *

